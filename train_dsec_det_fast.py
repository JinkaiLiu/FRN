import argparse
import collections
import numpy as np
import time
import math
import torch
import torch.optim as optim
from torch.cuda.amp import autocast, GradScaler
import os
from retinanet.dataloader_fast_combined import create_fast_dataloader as create_dsec_det_dataloader
from retinanet import model
from retinanet.csv_eval_dsec_det import evaluate, evaluate_coco_map

assert torch.__version__.split('.')[0] == '1'
print('CUDA available: {}'.format(torch.cuda.is_available()))


def time_since(since):
    now = time.time()
    s = now - since
    m = math.floor(s / 60)
    s -= m * 60
    return f'{m}m {s:.0f}s'


def normalize_event_data(img_event, method='tanh'):
    """å½’ä¸€åŒ–äº‹ä»¶æ•°æ®"""
    if method == 'tanh':
        # ä½¿ç”¨tanhå½’ä¸€åŒ–åˆ°[-1, 1]
        img_event = torch.tanh(img_event / 5.0)
    elif method == 'clip':
        # ç®€å•æˆªæ–­åˆ°[-2, 2]
        img_event = torch.clamp(img_event, -2, 2)
    elif method == 'minmax':
        # æœ€å°æœ€å¤§å½’ä¸€åŒ–
        min_val = img_event.min()
        max_val = img_event.max()
        if max_val > min_val:
            img_event = 2 * (img_event - min_val) / (max_val - min_val) - 1
    
    return img_event


def validate_annotations(annot, img_width=320, img_height=215):
    """éªŒè¯å’Œä¿®å¤æ ‡æ³¨"""
    if annot.shape[0] == 0:
        return annot
    
    # æå–åæ ‡
    x1, y1, x2, y2, cls = annot[:, 0], annot[:, 1], annot[:, 2], annot[:, 3], annot[:, 4]
    
    # ä¿®å¤åæ ‡é¡ºåºï¼ˆç¡®ä¿x2>x1, y2>y1ï¼‰
    x1_new = torch.minimum(x1, x2)
    x2_new = torch.maximum(x1, x2)
    y1_new = torch.minimum(y1, y2)
    y2_new = torch.maximum(y1, y2)
    
    # ç¡®ä¿æœ€å°å°ºå¯¸
    min_size = 2.0
    width = x2_new - x1_new
    height = y2_new - y1_new
    
    # å¦‚æœå®½åº¦æˆ–é«˜åº¦å¤ªå°ï¼Œæ‰©å±•è¾¹ç•Œæ¡†
    small_width_mask = width < min_size
    small_height_mask = height < min_size
    
    if small_width_mask.any():
        expand_w = (min_size - width[small_width_mask]) / 2
        x1_new[small_width_mask] -= expand_w
        x2_new[small_width_mask] += expand_w
    
    if small_height_mask.any():
        expand_h = (min_size - height[small_height_mask]) / 2
        y1_new[small_height_mask] -= expand_h
        y2_new[small_height_mask] += expand_h
    
    # è¾¹ç•Œæ£€æŸ¥
    x1_new = torch.clamp(x1_new, 0, img_width - min_size)
    y1_new = torch.clamp(y1_new, 0, img_height - min_size)
    x2_new = torch.clamp(x2_new, min_size, img_width)
    y2_new = torch.clamp(y2_new, min_size, img_height)
    
    # æœ€ç»ˆæ£€æŸ¥ï¼šç¡®ä¿æœ‰æ•ˆçš„è¾¹ç•Œæ¡†
    valid_mask = (x2_new > x1_new) & (y2_new > y1_new) & (x2_new - x1_new >= 1) & (y2_new - y1_new >= 1)
    
    if not valid_mask.all():
        print(f"[WARNING] Removing {(~valid_mask).sum()} invalid annotations")
    
    # é‡æ„æ ‡æ³¨
    fixed_annot = torch.stack([x1_new, y1_new, x2_new, y2_new, cls], dim=1)
    
    # åªä¿ç•™æœ‰æ•ˆçš„æ ‡æ³¨
    fixed_annot = fixed_annot[valid_mask]
    
    return fixed_annot


def debug_batch_data(data_batch, batch_idx):
    """ä¿®å¤ç‰ˆæœ¬çš„æ‰¹æ¬¡æ•°æ®è°ƒè¯•"""
    print(f"\n=== Debug Batch {batch_idx} (FIXED) ===")
    
    img_event = data_batch['img']
    img_rgb = data_batch['img_rgb']
    annot = data_batch['annot']
    
    batch_size = img_event.shape[0]
    print(f"æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"äº‹ä»¶æ•°æ®å½¢çŠ¶: {img_event.shape}, ç±»å‹: {img_event.dtype}")
    print(f"RGBæ•°æ®å½¢çŠ¶: {img_rgb.shape}, ç±»å‹: {img_rgb.dtype}")
    print(f"æ ‡æ³¨å½¢çŠ¶: {annot.shape}, ç±»å‹: {annot.dtype}")
    
    # æ£€æŸ¥ç¬¬ä¸€ä¸ªæ ·æœ¬
    sample_event = img_event[0]
    sample_rgb = img_rgb[0]
    sample_annot = annot[0]
    
    print(f"ç¬¬ä¸€ä¸ªæ ·æœ¬ - äº‹ä»¶: {sample_event.shape}, RGB: {sample_rgb.shape}, æ ‡æ³¨: {sample_annot.shape}")
    
    # å½’ä¸€åŒ–äº‹ä»¶æ•°æ®
    if sample_event.abs().max() > 5:
        print(f"äº‹ä»¶æ•°æ®èŒƒå›´è¿‡å¤§: [{sample_event.min():.3f}, {sample_event.max():.3f}]ï¼Œè¿›è¡Œå½’ä¸€åŒ–...")
        sample_event_norm = normalize_event_data(sample_event, method='tanh')
        print(f"å½’ä¸€åŒ–åäº‹ä»¶èŒƒå›´: [{sample_event_norm.min():.3f}, {sample_event_norm.max():.3f}]")
    else:
        print(f"äº‹ä»¶æ•°æ®èŒƒå›´: [{sample_event.min():.3f}, {sample_event.max():.3f}]")
    
    print(f"RGBæ•°æ®èŒƒå›´: [{sample_rgb.min():.3f}, {sample_rgb.max():.3f}]")
    
    # ä¿®å¤æ ‡æ³¨
    if sample_annot.shape[0] > 0:
        print(f"\nä¿®å¤å‰æ ‡æ³¨:")
        valid_indices = torch.where(sample_annot[:, 0] != -1)[0]
        for i, ann_idx in enumerate(valid_indices[:3]):
            ann = sample_annot[ann_idx]
            x1, y1, x2, y2, cls = ann[0].item(), ann[1].item(), ann[2].item(), ann[3].item(), ann[4].item()
            w, h = x2 - x1, y2 - y1
            print(f"  æ ‡æ³¨ {i}: [{x1:.1f}, {y1:.1f}, {x2:.1f}, {y2:.1f}] w={w:.1f} h={h:.1f} cls={int(cls)}")
        
        # åº”ç”¨ä¿®å¤
        fixed_annot = validate_annotations(sample_annot)
        
        print(f"\nä¿®å¤åæ ‡æ³¨ ({len(fixed_annot)} ä¸ªæœ‰æ•ˆ):")
        for i, ann in enumerate(fixed_annot[:3]):
            x1, y1, x2, y2, cls = ann[0].item(), ann[1].item(), ann[2].item(), ann[3].item(), ann[4].item()
            w, h = x2 - x1, y2 - y1
            print(f"  æ ‡æ³¨ {i}: [{x1:.1f}, {y1:.1f}, {x2:.1f}, {y2:.1f}] w={w:.1f} h={h:.1f} cls={int(cls)}")
            
            # éªŒè¯ä¿®å¤ç»“æœ
            if x2 <= x1 or y2 <= y1 or w < 1 or h < 1:
                print(f"    âŒ ä»ç„¶æœ‰é—®é¢˜!")
            else:
                print(f"    âœ… å·²ä¿®å¤")
    
    # æ€»ç»“
    issues = []
    
    # æ£€æŸ¥æ•°æ®èŒƒå›´
    if sample_event.abs().max() > 5:
        issues.append("äº‹ä»¶æ•°æ®èŒƒå›´è¿‡å¤§(å¯è‡ªåŠ¨ä¿®å¤)")
    if sample_rgb.min() < 0 or sample_rgb.max() > 1.5:
        issues.append("RGBæ•°æ®èŒƒå›´å¼‚å¸¸")
    
    # æ£€æŸ¥æ ‡æ³¨
    if sample_annot.shape[0] > 0:
        valid_annot = validate_annotations(sample_annot)
        if len(valid_annot) == 0:
            issues.append("æ²¡æœ‰æœ‰æ•ˆæ ‡æ³¨")
        elif len(valid_annot) < sample_annot.shape[0]:
            issues.append(f"éƒ¨åˆ†æ ‡æ³¨æ— æ•ˆ(å·²ä¿®å¤: {len(valid_annot)}/{sample_annot.shape[0]})")
    
    if issues:
        print(f"\nâš ï¸  å‘ç° {len(issues)} ä¸ªé—®é¢˜:")
        for issue in issues:
            print(f"  - {issue}")
        return True  # æ”¹ä¸ºTrueï¼Œå› ä¸ºé—®é¢˜å¯ä»¥ä¿®å¤
    else:
        print("\nâœ… æ‰¹æ¬¡æ•°æ®æ­£å¸¸")
        return True


def safe_training_step(retinanet, data, optimizer, iter_num, loss_threshold=50.0,
                      scaler=None, use_amp=False):
    """ä¿®å¤ç‰ˆçš„å®‰å…¨è®­ç»ƒæ­¥éª¤"""
    try:
        # GPUæ•°æ®ä¼ è¾“
        if isinstance(data['img_rgb'], list):
            img_rgb = torch.stack(data['img_rgb']).cuda(non_blocking=True).float()
        else:
            img_rgb = data['img_rgb'].cuda(non_blocking=True).float()
    
        if isinstance(data['img'], list):
            img_event = torch.stack(data['img']).cuda(non_blocking=True).float()
        else:
            img_event = data['img'].cuda(non_blocking=True).float()
        
        # å½’ä¸€åŒ–äº‹ä»¶æ•°æ®
        if img_event.abs().max() > 5:
            img_event = normalize_event_data(img_event, method='tanh')
        
        # å¤„ç†æ ‡æ³¨
        if isinstance(data['annot'], list):
            fixed_annots = []
            max_annots = 0
            
            for a in data['annot']:
                fixed_a = validate_annotations(a)
                fixed_annots.append(fixed_a)
                max_annots = max(max_annots, len(fixed_a))
            
            # å¡«å……åˆ°ç›¸åŒé•¿åº¦
            if max_annots == 0:
                max_annots = 1
            
            padded_annots = []
            for fixed_a in fixed_annots:
                if len(fixed_a) == 0:
                    padded = torch.ones((max_annots, 5), dtype=torch.float32) * -1
                elif len(fixed_a) < max_annots:
                    pad_len = max_annots - len(fixed_a)
                    pad = torch.ones((pad_len, 5), dtype=fixed_a.dtype) * -1
                    padded = torch.cat([fixed_a, pad], dim=0)
                else:
                    padded = fixed_a[:max_annots]
                padded_annots.append(padded)
            
            annot = torch.stack(padded_annots).cuda(non_blocking=True).float()
        else:
            # ä¿®å¤æ‰¹æ¬¡ä¸­çš„æ¯ä¸ªæ ‡æ³¨
            batch_size = data['annot'].shape[0]
            fixed_batch_annots = []
            
            for i in range(batch_size):
                fixed_annot = validate_annotations(data['annot'][i])
                fixed_batch_annots.append(fixed_annot)
            
            # æ‰¾åˆ°æœ€å¤§æ ‡æ³¨æ•°
            max_annots = max([len(a) for a in fixed_batch_annots]) if fixed_batch_annots else 1
            if max_annots == 0:
                max_annots = 1
            
            # å¡«å……
            padded_annots = []
            for fixed_a in fixed_batch_annots:
                if len(fixed_a) == 0:
                    padded = torch.ones((max_annots, 5), dtype=torch.float32) * -1
                elif len(fixed_a) < max_annots:
                    pad_len = max_annots - len(fixed_a)
                    pad = torch.ones((pad_len, 5), dtype=fixed_a.dtype) * -1
                    padded = torch.cat([fixed_a, pad], dim=0)
                else:
                    padded = fixed_a[:max_annots]
                padded_annots.append(padded)
            
            annot = torch.stack(padded_annots).cuda(non_blocking=True).float()
        
        # æ•°æ®éªŒè¯
        if torch.isnan(img_event).any() or torch.isinf(img_event).any():
            print(f"è¿­ä»£ {iter_num}: äº‹ä»¶æ•°æ®ä¸­æœ‰NaN/Inf")
            return None, None, None
        if torch.isnan(img_rgb).any() or torch.isinf(img_rgb).any():
            print(f"è¿­ä»£ {iter_num}: RGBæ•°æ®ä¸­æœ‰NaN/Inf")
            return None, None, None
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆæ ‡æ³¨
        valid_annot_mask = annot[:, :, 0] != -1
        if not valid_annot_mask.any():
            print(f"è¿­ä»£ {iter_num}: æ²¡æœ‰æœ‰æ•ˆæ ‡æ³¨ï¼Œè·³è¿‡")
            return None, None, None
        
        # æ¸…é›¶æ¢¯åº¦
        optimizer.zero_grad()
        
        # å‰å‘ä¼ æ’­
        if use_amp and scaler is not None:
            with autocast():
                classification_loss, regression_loss = retinanet([img_rgb, img_event, annot])
                classification_loss = classification_loss.mean()
                regression_loss = regression_loss.mean()
                total_loss = classification_loss + regression_loss
        else:
            classification_loss, regression_loss = retinanet([img_rgb, img_event, annot])
            classification_loss = classification_loss.mean()
            regression_loss = regression_loss.mean()
            total_loss = classification_loss + regression_loss
        
        # æŸå¤±éªŒè¯
        if torch.isnan(total_loss) or torch.isinf(total_loss):
            print(f"è¿­ä»£ {iter_num}: æŸå¤±NaN/Inf - åˆ†ç±»:{classification_loss.item():.6f}, å›å½’:{regression_loss.item():.6f}")
            return None, None, None
            
        if total_loss.item() > loss_threshold:
            print(f"è¿­ä»£ {iter_num}: æŸå¤±è¿‡å¤§ {total_loss.item():.1f} > {loss_threshold}")
            return None, None, None
        
        # åå‘ä¼ æ’­
        if use_amp and scaler is not None:
            scaler.scale(total_loss).backward()
            scaler.unscale_(optimizer)
            grad_norm = torch.nn.utils.clip_grad_norm_(retinanet.parameters(), 1.0)
            scaler.step(optimizer)
            scaler.update()
        else:
            total_loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(retinanet.parameters(), 1.0)
            optimizer.step()
        
        # æˆåŠŸæ—¥å¿—
        if iter_num % 50 == 0:
            print(f"è¿­ä»£ {iter_num}: æˆåŠŸ - æŸå¤±={total_loss.item():.4f}")
            valid_annots = valid_annot_mask.sum().item()
            print(f"  æœ‰æ•ˆæ ‡æ³¨: {valid_annots}, äº‹ä»¶èŒƒå›´: [{img_event.min():.3f}, {img_event.max():.3f}]")
        
        return classification_loss.item(), regression_loss.item(), total_loss.item()
        
    except Exception as e:
        print(f"è¿­ä»£ {iter_num}: è®­ç»ƒé”™è¯¯ - {e}")
        import traceback
        traceback.print_exc()
        return None, None, None


def train_epoch(dataloader, retinanet, optimizer, epoch_num, start_time, loss_threshold=50.0,
               scaler=None, use_amp=False):
    """è®­ç»ƒä¸€ä¸ªè½®æ¬¡"""
    retinanet.train()
    if hasattr(retinanet.module, 'freeze_bn'):
        retinanet.module.freeze_bn()
    
    loss_hist = collections.deque(maxlen=100)
    epoch_losses = []
    valid_iterations = 0
    total_iterations = 0
    
    for iter_num, data in enumerate(dataloader):
        total_iterations += 1
        
        cls_loss, reg_loss, total_loss = safe_training_step(
            retinanet, data, optimizer, iter_num, loss_threshold, scaler, use_amp
        )
        
        if total_loss is not None:
            valid_iterations += 1
            loss_hist.append(total_loss)
            
            # å®šæœŸæ—¥å¿—è®°å½•
            if iter_num % 10 == 0:
                avg_loss = np.mean(loss_hist) if loss_hist else 0
                valid_rate = 100 * valid_iterations / total_iterations
                print(f'[{time_since(start_time)}] ç¬¬ {epoch_num} è½® | è¿­ä»£ {iter_num} | '
                      f'æŸå¤±: åˆ†ç±»={cls_loss:.4f} å›å½’={reg_loss:.4f} æ€»è®¡={total_loss:.4f} | '
                      f'å¹³å‡: {avg_loss:.4f} | æœ‰æ•ˆ: {valid_iterations}/{total_iterations} ({valid_rate:.1f}%)')
                epoch_losses.append(avg_loss)
        
        # å†…å­˜æ¸…ç†
        if iter_num > 0 and iter_num % 100 == 0:
            torch.cuda.empty_cache()
    
    # è½®æ¬¡æ€»ç»“
    if epoch_losses:
        avg_epoch_loss = np.mean(epoch_losses)
        valid_rate = 100 * valid_iterations / total_iterations
        print(f"ç¬¬ {epoch_num} è½®æ€»ç»“: {valid_iterations}/{total_iterations} æœ‰æ•ˆ ({valid_rate:.1f}%), å¹³å‡æŸå¤±: {avg_epoch_loss:.4f}")
        return avg_epoch_loss
    else:
        print(f"ç¬¬ {epoch_num} è½®: æ²¡æœ‰æœ‰æ•ˆè¿­ä»£!")
        return float('inf')


def evaluate_epoch(retinanet, dataset_val, epoch_num, save_folder):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    print(f"\n{'='*20} éªŒè¯ç¬¬ {epoch_num} è½® {'='*20}")
    
    eval_save_folder = os.path.join(save_folder, f'eval_epoch_{epoch_num}')
    os.makedirs(eval_save_folder, exist_ok=True)
    
    mean_ap = evaluate(
        generator=dataset_val,
        retinanet=retinanet,
        iou_threshold=0.5,
        score_threshold=0.05,
        max_detections=100,
        save_detection=True,
        save_folder=eval_save_folder,
        load_detection=False,
        save_path=eval_save_folder
    )
    
    print(f"\nç¬¬ {epoch_num} è½®éªŒè¯ç»“æœ:")
    print(f"mAP@0.5: {mean_ap:.4f}")
    
    return mean_ap


def main():
    parser = argparse.ArgumentParser(description='DSECæ£€æµ‹è®­ç»ƒè„šæœ¬(ä¿®å¤ç‰ˆ)')
    
    # æ•°æ®å‚æ•°
    parser.add_argument('--root_dir', default='/media/data/hucao/zhenwu/hucao/DSEC/DSEC_Det', 
                       help='DSECæ•°æ®é›†æ ¹ç›®å½•')
    parser.add_argument('--batch_size', type=int, default=2, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--num_workers', type=int, default=8, help='æ•°æ®åŠ è½½å·¥ä½œè¿›ç¨‹æ•°')
    
    # è®­ç»ƒå‚æ•°  
    parser.add_argument('--epochs', type=int, default=60, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--learning_rate', type=float, default=5e-5, help='å­¦ä¹ ç‡')
    parser.add_argument('--loss_threshold', type=float, default=50.0, help='æŸå¤±é˜ˆå€¼')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--fusion', default='fpn_fusion', choices=['fpn_fusion', 'rgb', 'event'],
                       help='èåˆæ¨¡å‹ç±»å‹')
    parser.add_argument('--depth', type=int, default=50, choices=[18, 34, 50], 
                       help='ResNetæ·±åº¦')
    
    # è¯„ä¼°å‚æ•°
    parser.add_argument('--eval_interval', type=int, default=5, help='è¯„ä¼°é—´éš”(è½®æ•°)')
    parser.add_argument('--eval_coco', action='store_true', help='åŒæ—¶è¯„ä¼°COCOé£æ ¼çš„mAP')
    
    # ä¼˜åŒ–å‚æ•°
    parser.add_argument('--use_amp', action='store_true', help='ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦')
    
    # å…¶ä»–å‚æ•°
    parser.add_argument('--debug_data', action='store_true', help='å¯ç”¨æ•°æ®è°ƒè¯•')
    parser.add_argument('--continue_training', action='store_true', help='ä»æ£€æŸ¥ç‚¹ç»§ç»­è®­ç»ƒ')
    parser.add_argument('--checkpoint', default='', help='æ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--save_dir', default='/media/data/hucao/zehua/results_dsec/fixed_version',
                       help='ä¿å­˜æ£€æŸ¥ç‚¹çš„ç›®å½•')
    
    args = parser.parse_args()
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(args.save_dir, exist_ok=True)
    
    print("="*60)
    print("DSECæ£€æµ‹è®­ç»ƒè„šæœ¬(ä¿®å¤ç‰ˆ)")
    print("="*60)
    print(f"æ ¹ç›®å½•: {args.root_dir}")
    print(f"æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    print(f"å·¥ä½œè¿›ç¨‹æ•°: {args.num_workers}")
    print(f"å­¦ä¹ ç‡: {args.learning_rate}")
    print(f"æŸå¤±é˜ˆå€¼: {args.loss_threshold}")
    print(f"è®­ç»ƒè½®æ•°: {args.epochs}")
    print(f"èåˆæ¨¡å‹: {args.fusion}")
    print(f"æ··åˆç²¾åº¦: {args.use_amp}")
    print(f"è¯„ä¼°é—´éš”: {args.eval_interval}")
    print(f"COCOè¯„ä¼°: {args.eval_coco}")
    
    # åŠ è½½è®­ç»ƒæ•°æ®
    print("\nåŠ è½½è®­ç»ƒæ•°æ®é›†...")
    dataloader_train, dataset_train = create_dsec_det_dataloader(
        root_dir=args.root_dir,
        split='train',
        batch_size=args.batch_size,
        num_workers=0 if args.debug_data else args.num_workers
    )
    
    print(f"è®­ç»ƒæ•°æ®é›†å·²åŠ è½½: {len(dataset_train)} æ ·æœ¬")
    
    # åŠ è½½éªŒè¯æ•°æ®
    print("åŠ è½½éªŒè¯æ•°æ®é›†...")
    _, dataset_val = create_dsec_det_dataloader(
        root_dir=args.root_dir,
        split='val',  
        batch_size=1,  # éªŒè¯ä½¿ç”¨æ‰¹æ¬¡å¤§å°1
        num_workers=1
    )
    
    print(f"éªŒè¯æ•°æ®é›†å·²åŠ è½½: {len(dataset_val)} æ ·æœ¬")
    
    # è°ƒè¯•æ¨¡å¼
    if args.debug_data:
        print("\n" + "="*40)
        print("æ•°æ®è°ƒè¯•æ¨¡å¼(ä¿®å¤ç‰ˆ)")
        print("="*40)
        
        for batch_idx, data in enumerate(dataloader_train):
            success = debug_batch_data(data, batch_idx)
            if not success:
                print(f"âŒ æ‰¹æ¬¡ {batch_idx} æœ‰æ— æ³•ä¿®å¤çš„é—®é¢˜!")
            if batch_idx >= 4:  # æ£€æŸ¥å‰5ä¸ªbatch
                break
        
        print("\nè°ƒè¯•å®Œæˆã€‚ä½¿ç”¨ä¸å¸¦ --debug_data çš„å‘½ä»¤å¼€å§‹è®­ç»ƒã€‚")
        return
    
    # åˆ›å»ºæ¨¡å‹
    print("\nåˆ›å»ºæ¨¡å‹...")
    retinanet = model.resnet50(
        dataset_name='dsec',
        num_classes=dataset_train.num_classes,
        fusion_model=args.fusion,
        pretrained=True  # ä½¿ç”¨é¢„è®­ç»ƒæƒé‡
    )
    print(f"æ¨¡å‹å·²åˆ›å»º: ResNet{args.depth} å¸¦ {args.fusion} èåˆ")
    print(f"ç±»åˆ«æ•°: {dataset_train.num_classes}")
    
    # GPUè®¾ç½®
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    if torch.cuda.is_available():
        retinanet = retinanet.cuda()
        retinanet = torch.nn.DataParallel(retinanet).cuda()
        print("æ¨¡å‹å·²ç§»è‡³GPU")
    else:
        retinanet = torch.nn.DataParallel(retinanet)
        print("ä½¿ç”¨CPU")
    
    # ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
    optimizer = optim.Adam(retinanet.parameters(), lr=args.learning_rate)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, verbose=True, factor=0.5)
    
    # æ··åˆç²¾åº¦ç¼©æ”¾å™¨
    scaler = GradScaler() if args.use_amp else None
    
    # è·Ÿè¸ªæœ€ä½³æ€§èƒ½
    best_map = 0.0
    best_epoch = 0
    
    # è®­ç»ƒæ—¥å¿—
    train_log = []
    
    # åŠ è½½æ£€æŸ¥ç‚¹
    start_epoch = 0
    if args.continue_training and args.checkpoint:
        print(f"\nåŠ è½½æ£€æŸ¥ç‚¹: {args.checkpoint}")
        try:
            checkpoint = torch.load(args.checkpoint)
            retinanet.module.load_state_dict(checkpoint['model_state_dict'])
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            if scaler and 'scaler_state_dict' in checkpoint:
                scaler.load_state_dict(checkpoint['scaler_state_dict'])
            start_epoch = checkpoint.get('epoch', 0)
            best_map = checkpoint.get('best_map', 0.0)
            best_epoch = checkpoint.get('best_epoch', 0)
            print(f"ä»ç¬¬ {start_epoch} è½®æ¢å¤ï¼Œæœ€ä½³mAP: {best_map:.4f} (ç¬¬ {best_epoch} è½®)")
        except Exception as e:
            print(f"åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
            start_epoch = 0
    
    # å¼€å§‹è®­ç»ƒ
    print(f"\n" + "="*60)
    print("å¼€å§‹è®­ç»ƒ(ä¿®å¤ç‰ˆ)")
    print("="*60)
    
    start_time = time.time()
    
    for epoch in range(start_epoch, args.epochs):
        print(f"\n--- ç¬¬ {epoch+1}/{args.epochs} è½® ---")
        
        # è®­ç»ƒ
        avg_epoch_loss = train_epoch(
            dataloader_train, retinanet, optimizer, epoch, 
            start_time, args.loss_threshold, scaler, args.use_amp
        )
        
        # éªŒè¯
        current_map = 0.0
        if (epoch + 1) % args.eval_interval == 0 or epoch == args.epochs - 1:
            current_map = evaluate_epoch(retinanet, dataset_val, epoch, args.save_dir)
            
            # å¯é€‰çš„COCOè¯„ä¼°
            if args.eval_coco:
                print("è®¡ç®—COCOé£æ ¼çš„mAP...")
                coco_save_folder = os.path.join(args.save_dir, f'coco_eval_epoch_{epoch}')
                os.makedirs(coco_save_folder, exist_ok=True)
                
                coco_aps = evaluate_coco_map(
                    generator=dataset_val,
                    retinanet=retinanet,
                    iou_threshold=0.5,
                    score_threshold=0.05,
                    max_detections=100,
                    save_detection=True,
                    save_folder=coco_save_folder,
                    load_detection=False,
                    save_path=coco_save_folder
                )
                
                # è®¡ç®—COCOé£æ ¼çš„å¹³å‡mAP
                coco_map = np.mean([np.mean(aps) for aps in coco_aps.values()])
                print(f"COCOé£æ ¼mAP: {coco_map:.4f}")
            
            # æ›´æ–°æœ€ä½³ç»“æœ
            if current_map > best_map:
                best_map = current_map
                best_epoch = epoch
                print(f"ğŸ‰ æ–°çš„æœ€ä½³mAP: {best_map:.4f} (ç¬¬ {best_epoch} è½®)")
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                best_model_path = f'{args.save_dir}/best_model_fixed.pt'
                save_dict = {
                    'epoch': epoch,
                    'model_state_dict': retinanet.module.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'loss': avg_epoch_loss,
                    'map': current_map,
                    'best_map': best_map,
                    'best_epoch': best_epoch,
                    'config': {
                        'fusion': args.fusion,
                        'depth': args.depth,
                        'learning_rate': args.learning_rate,
                        'batch_size': args.batch_size,
                        'loss_threshold': args.loss_threshold
                    }
                }
                if scaler:
                    save_dict['scaler_state_dict'] = scaler.state_dict()
                
                torch.save(save_dict, best_model_path)
                print(f"æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {best_model_path}")
        
        # è®°å½•è®­ç»ƒè¿›åº¦
        train_log.append({
            'epoch': epoch,
            'loss': avg_epoch_loss,
            'map': current_map,
            'best_map': best_map,
            'best_epoch': best_epoch
        })
        
        # è°ƒæ•´å­¦ä¹ ç‡
        if avg_epoch_loss != float('inf'):
            scheduler.step(avg_epoch_loss)
        
        # å®šæœŸä¿å­˜æ¨¡å‹
        if epoch % 5 == 0 or epoch == args.epochs - 1:
            save_path = f'{args.save_dir}/dsec_retinanet_fixed_epoch_{epoch}.pt'
            save_dict = {
                'epoch': epoch,
                'model_state_dict': retinanet.module.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': avg_epoch_loss,
                'map': current_map,
                'best_map': best_map,
                'best_epoch': best_epoch,
                'train_log': train_log,
                'config': {
                    'fusion': args.fusion,
                    'depth': args.depth,
                    'learning_rate': args.learning_rate,
                    'batch_size': args.batch_size,
                    'loss_threshold': args.loss_threshold
                }
            }
            if scaler:
                save_dict['scaler_state_dict'] = scaler.state_dict()
            
            torch.save(save_dict, save_path)
            print(f"æ¨¡å‹å·²ä¿å­˜: {save_path}")
    
    # æœ€ç»ˆä¿å­˜
    final_path = f'{args.save_dir}/dsec_retinanet_fixed_final.pt'
    final_dict = {
        'epoch': args.epochs,
        'model_state_dict': retinanet.module.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'train_log': train_log,
        'best_map': best_map,
        'best_epoch': best_epoch,
        'config': {
            'fusion': args.fusion,
            'depth': args.depth,
            'learning_rate': args.learning_rate,
            'batch_size': args.batch_size,
            'loss_threshold': args.loss_threshold
        }
    }
    if scaler:
        final_dict['scaler_state_dict'] = scaler.state_dict()
    
    torch.save(final_dict, final_path)
    
    total_time = time_since(start_time)
    print(f"\n" + "="*60)
    print(f"è®­ç»ƒå®Œæˆ! ç”¨æ—¶: {total_time}")
    print(f"æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_path}")
    print(f"æœ€ä½³mAP: {best_map:.4f} (ç¬¬ {best_epoch} è½®)")
    print("="*60)


if __name__ == '__main__':
    main()
