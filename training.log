nohup: ignoring input
CUDA available: True
============================================================
DSEC检测训练脚本(修复版)
============================================================
根目录: /media/data/hucao/zhenwu/hucao/DSEC/DSEC_Det
批次大小: 1
工作进程数: 4
学习率: 5e-06
损失阈值: 50.0
训练轮数: 5
融合模型: fpn_fusion
混合精度: False
评估间隔: 5
COCO评估: False

加载训练数据集...
[DEBUG] Creating DSEC dataset from /media/data/hucao/zhenwu/hucao/DSEC/DSEC_Det for split 'train'
[DEBUG] Loaded split config: ['train', 'val', 'test']
[INFO] Found 41 valid directories with timestamps.txt files.
Valid directories: ['thun_00_a', 'interlaken_00_c', 'interlaken_00_d', 'interlaken_00_e', 'interlaken_00_f', 'interlaken_00_g', 'zurich_city_00_a', 'zurich_city_00_b', 'zurich_city_01_a', 'zurich_city_01_b', 'zurich_city_01_c', 'zurich_city_01_d', 'zurich_city_01_e', 'zurich_city_01_f', 'zurich_city_02_a', 'zurich_city_02_b', 'zurich_city_02_c', 'zurich_city_02_d', 'zurich_city_02_e', 'zurich_city_03_a', 'zurich_city_04_a', 'zurich_city_04_b', 'zurich_city_04_c', 'zurich_city_04_d', 'zurich_city_04_e', 'zurich_city_04_f', 'zurich_city_05_a', 'zurich_city_05_b', 'zurich_city_06_a', 'zurich_city_07_a', 'zurich_city_08_a', 'zurich_city_09_a', 'zurich_city_09_b', 'zurich_city_09_c', 'zurich_city_09_d', 'zurich_city_09_e', 'zurich_city_10_a', 'zurich_city_10_b', 'zurich_city_11_a', 'zurich_city_11_b', 'zurich_city_11_c']
[DEBUG] Found directories: ['zurich_city_04_a', 'zurich_city_04_b', 'zurich_city_04_c', 'zurich_city_04_d', 'zurich_city_04_e', 'zurich_city_04_f', 'zurich_city_11_a', 'zurich_city_11_b', 'zurich_city_11_c', 'zurich_city_05_a', 'zurich_city_05_b', 'zurich_city_08_a', 'zurich_city_06_a', 'zurich_city_07_a', 'thun_00_a', 'interlaken_00_c', 'interlaken_00_d', 'interlaken_00_e', 'interlaken_00_f', 'interlaken_00_g', 'zurich_city_00_a', 'zurich_city_00_b', 'zurich_city_01_a', 'zurich_city_01_b', 'zurich_city_01_c', 'zurich_city_01_d', 'zurich_city_01_e', 'zurich_city_01_f', 'zurich_city_02_a', 'zurich_city_02_b', 'zurich_city_02_c', 'zurich_city_02_d', 'zurich_city_02_e', 'zurich_city_03_a', 'zurich_city_10_a', 'zurich_city_10_b', 'zurich_city_09_a', 'zurich_city_09_b', 'zurich_city_09_c', 'zurich_city_09_d', 'zurich_city_09_e']
[DEBUG] Directory zurich_city_04_a has valid images via 'timestamps' attribute
[INFO] Directory zurich_city_04_a appears valid. Events: True
[DEBUG] Directory zurich_city_04_b has valid images via 'timestamps' attribute
[INFO] Directory zurich_city_04_b appears valid. Events: True
[DEBUG] Directory zurich_city_04_c has valid images via 'timestamps' attribute
[INFO] Directory zurich_city_04_c appears valid. Events: True
[DEBUG] Directory zurich_city_04_d has valid images via 'timestamps' attribute
[INFO] Directory zurich_city_04_d appears valid. Events: True
[DEBUG] Directory zurich_city_04_e has valid images via 'timestamps' attribute
[INFO] Directory zurich_city_04_e appears valid. Events: True
[DEBUG] Directory zurich_city_04_f has valid images via 'timestamps' attribute
[INFO] Directory zurich_city_04_f appears valid. Events: True
[DEBUG] Directory zurich_city_11_a has valid images via 'timestamps' attribute
[INFO] Directory zurich_city_11_a appears valid. Events: True
[DEBUG] Directory zurich_city_11_b has valid images via 'timestamps' attribute
[INFO] Directory zurich_city_11_b appears valid. Events: True
[DEBUG] Directory zurich_city_11_c has valid images via 'timestamps' attribute
[INFO] Directory zurich_city_11_c appears valid. Events: True
[DEBUG] Directory zurich_city_05_a has valid images via 'timestamps' attribute
[INFO] Directory zurich_city_05_a appears valid. Events: True
[DEBUG] Directory zurich_city_05_b has valid images via 'timestamps' attribute
[INFO] Directory zurich_city_05_b appears valid. Events: True
[DEBUG] Directory zurich_city_08_a has valid images via 'timestamps' attribute
[INFO] Directory zurich_city_08_a appears valid. Events: True
[DEBUG] Directory zurich_city_06_a has valid images via 'timestamps' attribute
[INFO] Directory zurich_city_06_a appears valid. Events: True
[DEBUG] Directory zurich_city_07_a has valid images via 'timestamps' attribute
[INFO] Directory zurich_city_07_a appears valid. Events: True
[DEBUG] Directory thun_00_a has valid images via 'timestamps' attribute
[INFO] Directory thun_00_a appears valid. Events: True
[DEBUG] Directory interlaken_00_c has valid images via 'timestamps' attribute
[INFO] Directory interlaken_00_c appears valid. Events: True
[DEBUG] Directory interlaken_00_d has valid images via 'timestamps' attribute
[INFO] Directory interlaken_00_d appears valid. Events: True
[DEBUG] Directory interlaken_00_e has valid images via 'timestamps' attribute
[INFO] Directory interlaken_00_e appears valid. Events: True
[DEBUG] Directory interlaken_00_f has valid images via 'timestamps' attribute
[INFO] Directory interlaken_00_f appears valid. Events: True
[DEBUG] Directory interlaken_00_g has valid images via 'timestamps' attribute
[INFO] Directory interlaken_00_g appears valid. Events: True
[DEBUG] Directory zurich_city_00_a has valid images via 'timestamps' attribute
[INFO] Directory zurich_city_00_a appears valid. Events: True
[DEBUG] Directory zurich_city_00_b has valid images via 'timestamps' attribute
[INFO] Directory zurich_city_00_b appears valid. Events: True
[DEBUG] Directory zurich_city_01_a has valid images via 'timestamps' attribute
[INFO] Directory zurich_city_01_a appears valid. Events: True
[DEBUG] Directory zurich_city_01_b has valid images via 'timestamps' attribute
[INFO] Directory zurich_city_01_b appears valid. Events: True
[DEBUG] Directory zurich_city_01_c has valid images via 'timestamps' attribute
[INFO] Directory zurich_city_01_c appears valid. Events: True
[DEBUG] Directory zurich_city_01_d has valid images via 'timestamps' attribute
[INFO] Directory zurich_city_01_d appears valid. Events: True
[DEBUG] Directory zurich_city_01_e has valid images via 'timestamps' attribute
[INFO] Directory zurich_city_01_e appears valid. Events: True
[DEBUG] Directory zurich_city_01_f has valid images via 'timestamps' attribute
[INFO] Directory zurich_city_01_f appears valid. Events: True
[DEBUG] Directory zurich_city_02_a has valid images via 'timestamps' attribute
[INFO] Directory zurich_city_02_a appears valid. Events: True
[DEBUG] Directory zurich_city_02_b has valid images via 'timestamps' attribute
[INFO] Directory zurich_city_02_b appears valid. Events: True
[DEBUG] Directory zurich_city_02_c has valid images via 'timestamps' attribute
[INFO] Directory zurich_city_02_c appears valid. Events: True
[DEBUG] Directory zurich_city_02_d has valid images via 'timestamps' attribute
[INFO] Directory zurich_city_02_d appears valid. Events: True
[DEBUG] Directory zurich_city_02_e has valid images via 'timestamps' attribute
[INFO] Directory zurich_city_02_e appears valid. Events: True
[DEBUG] Directory zurich_city_03_a has valid images via 'timestamps' attribute
[INFO] Directory zurich_city_03_a appears valid. Events: True
[DEBUG] Directory zurich_city_10_a has valid images via 'timestamps' attribute
[INFO] Directory zurich_city_10_a appears valid. Events: True
[DEBUG] Directory zurich_city_10_b has valid images via 'timestamps' attribute
[INFO] Directory zurich_city_10_b appears valid. Events: True
[DEBUG] Directory zurich_city_09_a has valid images via 'timestamps' attribute
[INFO] Directory zurich_city_09_a appears valid. Events: True
[DEBUG] Directory zurich_city_09_b has valid images via 'timestamps' attribute
[INFO] Directory zurich_city_09_b appears valid. Events: True
[DEBUG] Directory zurich_city_09_c has valid images via 'timestamps' attribute
[INFO] Directory zurich_city_09_c appears valid. Events: True
[DEBUG] Directory zurich_city_09_d has valid images via 'timestamps' attribute
[INFO] Directory zurich_city_09_d appears valid. Events: True
[DEBUG] Directory zurich_city_09_e has valid images via 'timestamps' attribute
[INFO] Directory zurich_city_09_e appears valid. Events: True
[DEBUG] Initialized DSEC with root: /media/data/hucao/zhenwu/hucao/DSEC/DSEC_Det
[DEBUG] Available directories: ['zurich_city_04_a', 'zurich_city_04_b', 'zurich_city_04_c', 'zurich_city_04_d', 'zurich_city_04_e', 'zurich_city_04_f', 'zurich_city_11_a', 'zurich_city_11_b', 'zurich_city_11_c', 'zurich_city_05_a', 'zurich_city_05_b', 'zurich_city_08_a', 'zurich_city_06_a', 'zurich_city_07_a', 'thun_00_a', 'interlaken_00_c', 'interlaken_00_d', 'interlaken_00_e', 'interlaken_00_f', 'interlaken_00_g', 'zurich_city_00_a', 'zurich_city_00_b', 'zurich_city_01_a', 'zurich_city_01_b', 'zurich_city_01_c', 'zurich_city_01_d', 'zurich_city_01_e', 'zurich_city_01_f', 'zurich_city_02_a', 'zurich_city_02_b', 'zurich_city_02_c', 'zurich_city_02_d', 'zurich_city_02_e', 'zurich_city_03_a', 'zurich_city_10_a', 'zurich_city_10_b', 'zurich_city_09_a', 'zurich_city_09_b', 'zurich_city_09_c', 'zurich_city_09_d', 'zurich_city_09_e']
[DEBUG] DSEC dataset created successfully
[DEBUG] Dataset length: 47998
[DEBUG] Dataset classes: ('car', 'pedestrian')
[DEBUG] Image size: 480x640
[DEBUG] DSECWrapper initialized with 47998 samples
[DEBUG] DataLoader created successfully
  - Batch size: 1
  - Num workers: 0
  - Expected tensor sizes: Event[B,5,480,640], RGB[B,3,480,640]
训练数据集已加载: 47998 样本
加载验证数据集...
[DEBUG] Creating DSEC dataset from /media/data/hucao/zhenwu/hucao/DSEC/DSEC_Det for split 'val'
[DEBUG] Loaded split config: ['train', 'val', 'test']
[INFO] Found 6 valid directories with timestamps.txt files.
Valid directories: ['zurich_city_16_a', 'zurich_city_17_a', 'zurich_city_18_a', 'zurich_city_19_a', 'zurich_city_20_a', 'zurich_city_21_a']
[DEBUG] Found directories: ['zurich_city_21_a', 'zurich_city_17_a', 'zurich_city_20_a', 'zurich_city_16_a', 'zurich_city_19_a', 'zurich_city_18_a']
[DEBUG] Directory zurich_city_21_a has valid images via 'timestamps' attribute
[INFO] Directory zurich_city_21_a appears valid. Events: True
[DEBUG] Directory zurich_city_17_a has valid images via 'timestamps' attribute
[INFO] Directory zurich_city_17_a appears valid. Events: True
[DEBUG] Directory zurich_city_20_a has valid images via 'timestamps' attribute
[INFO] Directory zurich_city_20_a appears valid. Events: True
[DEBUG] Directory zurich_city_16_a has valid images via 'timestamps' attribute
[INFO] Directory zurich_city_16_a appears valid. Events: True
[DEBUG] Directory zurich_city_19_a has valid images via 'timestamps' attribute
[INFO] Directory zurich_city_19_a appears valid. Events: True
[DEBUG] Directory zurich_city_18_a has valid images via 'timestamps' attribute
[INFO] Directory zurich_city_18_a appears valid. Events: True
[DEBUG] Initialized DSEC with root: /media/data/hucao/zhenwu/hucao/DSEC/DSEC_Det
[DEBUG] Available directories: ['zurich_city_21_a', 'zurich_city_17_a', 'zurich_city_20_a', 'zurich_city_16_a', 'zurich_city_19_a', 'zurich_city_18_a']
[DEBUG] DSEC dataset created successfully
[DEBUG] Dataset length: 8097
[DEBUG] Dataset classes: ('car', 'pedestrian')
[DEBUG] Image size: 480x640
[DEBUG] DSECWrapper initialized with 8097 samples
[DEBUG] DataLoader created successfully
  - Batch size: 1
  - Num workers: 0
  - Expected tensor sizes: Event[B,5,480,640], RGB[B,3,480,640]
验证数据集已加载: 8097 样本

创建模型...
模型已创建: ResNet50 带 fpn_fusion 融合
类别数: 2
模型已移至GPU

============================================================
开始训练(修复版)
============================================================

--- 第 1/5 轮 ---
[DEBUG] __getitem__ called with index 39112
[DEBUG] Event count: 123215
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 7975.0, range: [-19.000, 31.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([1, 5])
[DEBUG] Final ranges - Event: [-0.999, 1.000], RGB: [0.000, 1.000]
迭代 0: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 5.78 GiB already allocated; 1.24 GiB free; 7.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 7057
[DEBUG] Event count: 107674
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 24884.0, range: [-13.000, 24.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([4, 5])
[DEBUG] Final ranges - Event: [-0.989, 1.000], RGB: [0.000, 1.000]
迭代 1: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 5.79 GiB already allocated; 1.30 GiB free; 6.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 1654
[DEBUG] Event count: 57431
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 11869.0, range: [-9.000, 16.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([7, 5])
[DEBUG] Final ranges - Event: [-0.947, 0.997], RGB: [0.000, 1.000]
迭代 2: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 5.79 GiB already allocated; 1.36 GiB free; 6.93 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 31887
[DEBUG] Event count: 45967
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 6243.0, range: [-10.000, 14.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([2, 5])
[DEBUG] Final ranges - Event: [-0.964, 0.993], RGB: [0.000, 1.000]
迭代 3: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 5.79 GiB already allocated; 1.38 GiB free; 6.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 8834
[DEBUG] Event count: 155443
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: -15631.0, range: [-10.000, 15.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([2, 5])
[DEBUG] Final ranges - Event: [-0.964, 0.995], RGB: [0.000, 0.992]
迭代 4: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 21.81 MiB free; 8.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 29904
[DEBUG] Event count: 71646
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 7724.0, range: [-13.000, 16.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([10, 5])
[DEBUG] Final ranges - Event: [-0.989, 0.997], RGB: [0.000, 1.000]
迭代 5: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 41.81 MiB free; 8.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 13384
[DEBUG] Event count: 53372
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 5.78 GiB already allocated; 1.24 GiB free; 7.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 5.79 GiB already allocated; 1.30 GiB free; 6.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 5.79 GiB already allocated; 1.36 GiB free; 6.93 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 5.79 GiB already allocated; 1.38 GiB free; 6.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 21.81 MiB free; 8.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 41.81 MiB free; 8.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 41.81 MiB free; 8.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 41.81 MiB free; 8.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 41.81 MiB free; 8.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
[DEBUG] Voxel shape: (5, 480, 640), sum: 19626.0, range: [-6.000, 18.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([13, 5])
[DEBUG] Final ranges - Event: [-0.834, 0.999], RGB: [0.000, 1.000]
迭代 6: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 41.81 MiB free; 8.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 23782
[DEBUG] Event count: 49819
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 10407.0, range: [-10.000, 41.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([7, 5])
[DEBUG] Final ranges - Event: [-0.964, 1.000], RGB: [0.000, 1.000]
迭代 7: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 41.81 MiB free; 8.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 16692
[DEBUG] Event count: 87575
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 29261.0, range: [-6.000, 11.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([1, 5])
[DEBUG] Final ranges - Event: [-0.834, 0.976], RGB: [0.000, 0.922]
迭代 8: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 41.81 MiB free; 8.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 27474
[DEBUG] Event count: 77486
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 3548.0, range: [-8.000, 33.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([6, 5])
[DEBUG] Final ranges - Event: [-0.922, 1.000], RGB: [0.000, 1.000]
迭代 9: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 41.81 MiB free; 8.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 217
[DEBUG] Event count: 92697
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 20171.0, range: [-10.000, 18.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([6, 5])
[DEBUG] Final ranges - Event: [-0.964, 0.999], RGB: [0.000, 1.000]
迭代 10: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 41.81 MiB free; 8.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 11071
[DEBUG] Event count: 27862
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 7556.0, range: [-7.000, 12.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([6, 5])
[DEBUG] Final ranges - Event: [-0.885, 0.984], RGB: [0.000, 1.000]
迭代 11: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 41.81 MiB free; 8.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 24614
[DEBUG] Event count: 69502
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 12726.0, range: [-17.000, 33.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([19, 5])
[DEBUG] Final ranges - Event: [-0.998, 1.000], RGB: [0.000, 1.000]
迭代 12: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 41.81 MiB free; 8.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 5890
[DEBUG] Event count: 56801
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 18311.0, range: [-10.000, 19.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([4, 5])
[DEBUG] Final ranges - Event: [-0.964, 0.999], RGB: [0.000, 1.000]
迭代 13: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 41.81 MiB free; 8.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 30125
[DEBUG] Event count: 45481
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 1589.0, range: [-11.000, 13.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([11, 5])
[DEBUG] Final ranges - Event: [-0.976, 0.989], RGB: [0.000, 1.000]
迭代 14: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 41.81 MiB free; 8.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 20037
[DEBUG] Event count: 34982
[DEBUG] x range: 0 - 319, y range: 6 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 14192.0, range: [-5.000, 13.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([2, 5])
[DEBUG] Final ranges - Event: [-0.762, 0.989], RGB: [0.000, 0.992]
迭代 15: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 41.81 MiB free; 8.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 43316
[DEBUG] Event count: 137843
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: -28309.0, range: [-16.000, 19.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([7, 5])
[DEBUG] Final ranges - Event: [-0.997, 0.999], RGB: [0.000, 1.000]
迭代 16: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 41.81 MiB free; 8.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 41.81 MiB free; 8.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 41.81 MiB free; 8.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 41.81 MiB free; 8.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 41.81 MiB free; 8.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 41.81 MiB free; 8.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 41.81 MiB free; 8.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 41.81 MiB free; 8.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 41.81 MiB free; 8.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 41.81 MiB free; 8.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
[DEBUG] __getitem__ called with index 28076
[DEBUG] Event count: 74380
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 14392.0, range: [-9.000, 44.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([10, 5])
[DEBUG] Final ranges - Event: [-0.947, 1.000], RGB: [0.000, 1.000]
迭代 17: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 41.81 MiB free; 8.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 41304
[DEBUG] Event count: 84066
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 9614.0, range: [-15.000, 21.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([7, 5])
[DEBUG] Final ranges - Event: [-0.995, 1.000], RGB: [0.000, 1.000]
迭代 18: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 41.81 MiB free; 8.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 2418
[DEBUG] Event count: 109639
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 19435.0, range: [-13.000, 22.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([5, 5])
[DEBUG] Final ranges - Event: [-0.989, 1.000], RGB: [0.000, 1.000]
迭代 19: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 41.81 MiB free; 8.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 46904
[DEBUG] Event count: 48383
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 6203.0, range: [-18.000, 24.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([7, 5])
[DEBUG] Final ranges - Event: [-0.999, 1.000], RGB: [0.000, 1.000]
迭代 20: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 41.81 MiB free; 8.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 30228
[DEBUG] Event count: 72021
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 12991.0, range: [-19.000, 44.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([5, 5])
[DEBUG] Final ranges - Event: [-0.999, 1.000], RGB: [0.000, 1.000]
迭代 21: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 41.81 MiB free; 8.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 30844
[DEBUG] Event count: 98731
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 7659.0, range: [-15.000, 19.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([9, 5])
[DEBUG] Final ranges - Event: [-0.995, 0.999], RGB: [0.000, 1.000]
迭代 22: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 41.81 MiB free; 8.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 24235
[DEBUG] Event count: 42361
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 7297.0, range: [-8.000, 35.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([8, 5])
[DEBUG] Final ranges - Event: [-0.922, 1.000], RGB: [0.000, 1.000]
迭代 23: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 41.81 MiB free; 8.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 6399
[DEBUG] Event count: 172535
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: -26825.0, range: [-12.000, 18.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([2, 5])
[DEBUG] Final ranges - Event: [-0.984, 0.999], RGB: [0.000, 1.000]
迭代 24: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 41.81 MiB free; 8.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 33448
[DEBUG] Event count: 97999
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 11261.0, range: [-19.000, 32.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([4, 5])
[DEBUG] Final ranges - Event: [-0.999, 1.000], RGB: [0.000, 1.000]
迭代 25: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 41.81 MiB free; 8.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 47418
[DEBUG] Event count: 52086
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 7842.0, range: [-15.000, 19.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([2, 5])
[DEBUG] Final ranges - Event: [-0.995, 0.999], RGB: [0.000, 1.000]
迭代 26: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 41.81 MiB free; 8.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 18967
[DEBUG] Event count: 78364
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 28358.0, range: [-7.000, 12.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([2, 5])
[DEBUG] Final ranges - Event: [-0.885, 0.984], RGB: [0.000, 1.000]
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 41.81 MiB free; 8.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 41.81 MiB free; 8.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 41.81 MiB free; 8.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 41.81 MiB free; 8.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 41.81 MiB free; 8.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 41.81 MiB free; 8.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 41.81 MiB free; 8.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 41.81 MiB free; 8.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 41.81 MiB free; 8.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 41.81 MiB free; 8.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 41.81 MiB free; 8.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 41.81 MiB free; 8.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 41.81 MiB free; 8.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 61.81 MiB free; 8.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
迭代 27: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 41.81 MiB free; 8.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 24174
[DEBUG] Event count: 23902
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 5972.0, range: [-7.000, 19.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([10, 5])
[DEBUG] Final ranges - Event: [-0.885, 0.999], RGB: [0.000, 1.000]
迭代 28: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 41.81 MiB free; 8.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 17485
[DEBUG] Event count: 72558
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 27346.0, range: [-5.000, 9.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([1, 5])
[DEBUG] Final ranges - Event: [-0.762, 0.947], RGB: [0.000, 0.922]
迭代 29: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 41.81 MiB free; 8.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 36383
[DEBUG] Event count: 110332
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 14624.0, range: [-12.000, 15.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([3, 5])
[DEBUG] Final ranges - Event: [-0.984, 0.995], RGB: [0.000, 1.000]
迭代 30: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 41.81 MiB free; 8.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 40381
[DEBUG] Event count: 78121
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 16913.0, range: [-10.000, 28.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([2, 5])
[DEBUG] Final ranges - Event: [-0.964, 1.000], RGB: [0.000, 1.000]
迭代 31: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 61.81 MiB free; 8.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 35194
[DEBUG] Event count: 144389
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: -31121.0, range: [-19.000, 16.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([1, 5])
[DEBUG] Final ranges - Event: [-0.999, 0.997], RGB: [0.000, 1.000]
迭代 32: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 10381
[DEBUG] Event count: 99966
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 28334.0, range: [-10.000, 30.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([1, 5])
[DEBUG] Final ranges - Event: [-0.964, 1.000], RGB: [0.000, 1.000]
迭代 33: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 13277
[DEBUG] Event count: 33554
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 8438.0, range: [-6.000, 12.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([3, 5])
[DEBUG] Final ranges - Event: [-0.834, 0.984], RGB: [0.000, 1.000]
迭代 34: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 27914
[DEBUG] Event count: 85808
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 14662.0, range: [-15.000, 66.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([14, 5])
[DEBUG] Final ranges - Event: [-0.995, 1.000], RGB: [0.000, 1.000]
迭代 35: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 47455
[DEBUG] Event count: 63350
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 2400.0, range: [-25.000, 24.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([3, 5])
[DEBUG] Final ranges - Event: [-1.000, 1.000], RGB: [0.000, 1.000]
迭代 36: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 45153
[DEBUG] Event count: 34472
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 4926.0, range: [-8.000, 41.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([3, 5])
[DEBUG] Final ranges - Event: [-0.922, 1.000], RGB: [0.000, 1.000]
迭代 37: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 16329
[DEBUG] Event count: 134038
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: -3180.0, range: [-7.000, 11.000]
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([4, 5])
[DEBUG] Final ranges - Event: [-0.885, 0.976], RGB: [0.000, 1.000]
迭代 38: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 11248
[DEBUG] Event count: 93181
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 26733.0, range: [-14.000, 27.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([9, 5])
[DEBUG] Final ranges - Event: [-0.993, 1.000], RGB: [0.000, 1.000]
迭代 39: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 6834
[DEBUG] Event count: 56868
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 10866.0, range: [-8.000, 17.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([7, 5])
[DEBUG] Final ranges - Event: [-0.922, 0.998], RGB: [0.000, 1.000]
迭代 40: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 1693
[DEBUG] Event count: 24648
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 2846.0, range: [-10.000, 13.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([4, 5])
[DEBUG] Final ranges - Event: [-0.964, 0.989], RGB: [0.000, 1.000]
迭代 41: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 22315
[DEBUG] Event count: 55030
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 19550.0, range: [-9.000, 14.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([11, 5])
[DEBUG] Final ranges - Event: [-0.947, 0.993], RGB: [0.000, 1.000]
迭代 42: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 1880
[DEBUG] Event count: 54730
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 12592.0, range: [-8.000, 12.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([9, 5])
[DEBUG] Final ranges - Event: [-0.922, 0.984], RGB: [0.000, 1.000]
迭代 43: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 37798
[DEBUG] Event count: 78492
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 4304.0, range: [-11.000, 25.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([1, 5])
[DEBUG] Final ranges - Event: [-0.976, 1.000], RGB: [0.000, 1.000]
迭代 44: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 11303
[DEBUG] Event count: 44127
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 9909.0, range: [-6.000, 12.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([2, 5])
[DEBUG] Final ranges - Event: [-0.834, 0.984], RGB: [0.000, 1.000]
迭代 45: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 28530
[DEBUG] Event count: 139554
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 16600.0, range: [-10.000, 40.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([7, 5])
[DEBUG] Final ranges - Event: [-0.964, 1.000], RGB: [0.000, 1.000]
迭代 46: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 47332
[DEBUG] Event count: 83355
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 14209.0, range: [-12.000, 22.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([2, 5])
[DEBUG] Final ranges - Event: [-0.984, 1.000], RGB: [0.000, 1.000]
迭代 47: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 46950
[DEBUG] Event count: 66016
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 13788.0, range: [-17.000, 33.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([9, 5])
[DEBUG] Final ranges - Event: [-0.998, 1.000], RGB: [0.000, 1.000]
迭代 48: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 21831
[DEBUG] Event count: 52794
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 20992.0, range: [-6.000, 13.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([3, 5])
[DEBUG] Final ranges - Event: [-0.834, 0.989], RGB: [0.000, 1.000]
迭代 49: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 43570
[DEBUG] Event count: 83938
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 2144.0, range: [-19.000, 28.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([1, 5])
[DEBUG] Final ranges - Event: [-0.999, 1.000], RGB: [0.000, 1.000]
迭代 50: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 14442
[DEBUG] Event count: 109469
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 31697.0, range: [-14.000, 21.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([1, 5])
[DEBUG] Final ranges - Event: [-0.993, 1.000], RGB: [0.000, 1.000]
迭代 51: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 35941
[DEBUG] Event count: 61383
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 14231.0, range: [-9.000, 23.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([5, 5])
[DEBUG] Final ranges - Event: [-0.947, 1.000], RGB: [0.000, 1.000]
迭代 52: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 3833
[DEBUG] Event count: 71413
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 18285.0, range: [-15.000, 27.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([5, 5])
[DEBUG] Final ranges - Event: [-0.995, 1.000], RGB: [0.000, 1.000]
迭代 53: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 34101
[DEBUG] Event count: 157995
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: -41119.0, range: [-7.000, 9.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([3, 5])
[DEBUG] Final ranges - Event: [-0.885, 0.947], RGB: [0.000, 1.000]
迭代 54: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 13970
[DEBUG] Event count: 70081
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 21147.0, range: [-8.000, 23.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([6, 5])
[DEBUG] Final ranges - Event: [-0.922, 1.000], RGB: [0.000, 1.000]
迭代 55: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 907
[DEBUG] Event count: 135573
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 13667.0, range: [-12.000, 17.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([3, 5])
[DEBUG] Final ranges - Event: [-0.984, 0.998], RGB: [0.000, 1.000]
迭代 56: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 13675
[DEBUG] Event count: 119139
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 26111.0, range: [-12.000, 19.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([5, 5])
[DEBUG] Final ranges - Event: [-0.984, 0.999], RGB: [0.000, 1.000]
迭代 57: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 13962
[DEBUG] Event count: 68207
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 24003.0, range: [-9.000, 24.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([5, 5])
[DEBUG] Final ranges - Event: [-0.947, 1.000], RGB: [0.000, 0.937]
迭代 58: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 40084
[DEBUG] Event count: 123226
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: -2478.0, range: [-16.000, 13.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([4, 5])
[DEBUG] Final ranges - Event: [-0.997, 0.989], RGB: [0.000, 1.000]
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
迭代 59: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 14790
[DEBUG] Event count: 121076
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 18472.0, range: [-9.000, 18.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([2, 5])
[DEBUG] Final ranges - Event: [-0.947, 0.999], RGB: [0.000, 1.000]
迭代 60: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 19999
[DEBUG] Event count: 26449
[DEBUG] x range: 0 - 319, y range: 36 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 8791.0, range: [-6.000, 14.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([1, 5])
[DEBUG] Final ranges - Event: [-0.834, 0.993], RGB: [0.000, 0.961]
迭代 61: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 38834
[DEBUG] Event count: 147337
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: -63.0, range: [-13.000, 26.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([3, 5])
[DEBUG] Final ranges - Event: [-0.989, 1.000], RGB: [0.000, 1.000]
迭代 62: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 31547
[DEBUG] Event count: 71956
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 10084.0, range: [-8.000, 21.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([3, 5])
[DEBUG] Final ranges - Event: [-0.922, 1.000], RGB: [0.000, 1.000]
迭代 63: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 12051
[DEBUG] Event count: 35826
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 9538.0, range: [-6.000, 10.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([4, 5])
[DEBUG] Final ranges - Event: [-0.834, 0.964], RGB: [0.000, 0.996]
迭代 64: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 40402
[DEBUG] Event count: 87183
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 14017.0, range: [-14.000, 18.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([3, 5])
[DEBUG] Final ranges - Event: [-0.993, 0.999], RGB: [0.000, 1.000]
迭代 65: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 20125
[DEBUG] Event count: 25378
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 11404.0, range: [-6.000, 13.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([3, 5])
[DEBUG] Final ranges - Event: [-0.834, 0.989], RGB: [0.000, 1.000]
迭代 66: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 11484
[DEBUG] Event count: 144088
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: -7474.0, range: [-12.000, 11.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([7, 5])
[DEBUG] Final ranges - Event: [-0.984, 0.976], RGB: [0.000, 1.000]
迭代 67: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 13154
[DEBUG] Event count: 36501
[DEBUG] x range: 0 - 319, y range: 0 - 238
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 15351.0, range: [-9.000, 19.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([1, 5])
[DEBUG] Final ranges - Event: [-0.947, 0.999], RGB: [0.000, 1.000]
迭代 68: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 10709
[DEBUG] Event count: 90783
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: 21213.0, range: [-12.000, 25.000]
[DEBUG] Final shapes - Event: torch.Size([5, 480, 640]), RGB: torch.Size([1, 3, 480, 640]), Annot: torch.Size([4, 5])
[DEBUG] Final ranges - Event: [-0.984, 1.000], RGB: [0.000, 1.000]
迭代 69: 训练错误 - CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[DEBUG] __getitem__ called with index 38160
[DEBUG] Event count: 161724
[DEBUG] x range: 0 - 319, y range: 0 - 239
[DEBUG] Target size: 480x640
[DEBUG] Voxel shape: (5, 480, 640), sum: -31628.0, range: [-12.000, 13.000]
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 23.70 GiB total capacity; 7.16 GiB already allocated; 81.81 MiB free; 8.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_dsec_det_fast.py", line 303, in safe_training_step
    total_loss.backward()
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/test7/miniconda3/envs/FRN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
